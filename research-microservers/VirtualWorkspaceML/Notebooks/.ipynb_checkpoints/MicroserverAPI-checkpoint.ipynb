{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15af77e0-203a-4b0e-9cd5-ff4cc9698046",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Virtual Workspace Web App Microserver API\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4aa1ca-e66f-4f36-94ca-362657b2deed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  Setup all lib and env:\n",
    "**Python Lib:**\n",
    ">* FastApi - *for client access* \n",
    ">* redis - *for base data*\n",
    ">* websockets - *for desktop control* \n",
    ">* aiortc - *for desktop streaming*\n",
    "\n",
    "```\n",
    "pip install fastapi\n",
    "pip install uvicorn\n",
    "pip install transformers\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "pip install sb3-contrib\n",
    "pip install redis\n",
    "pip install aioredis\n",
    "pip install python-socketio[client]\n",
    "pip install aiortc\n",
    "pip install av\n",
    "pip install aiohttp\n",
    "pip install transformers torch torchvision clip stable-baselines3 webrtcvad torchaudio\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10917ad7-1535-4185-bb00-a22065fdfd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if ffmpeg is installed\n",
    "try:\n",
    "    if os.name == 'nt':\n",
    "        subprocess.check_output(['where', 'ffmpeg'])\n",
    "    else:\n",
    "        subprocess.check_output(['ffmpeg', '-version'])\n",
    "    print('ffmpeg already installed')\n",
    "except subprocess.CalledProcessError:\n",
    "    # Install ffmpeg\n",
    "    print('ffmpeg not found. Installing...')\n",
    "    if os.name == 'nt':\n",
    "        ffmpeg_bin_dir = r'C:\\ProgramData\\chocolatey\\lib\\ffmpeg\\tools'\n",
    "        os.environ['PATH'] += os.pathsep + ffmpeg_bin_dir\n",
    "        if not os.path.exists(ffmpeg_bin_dir):\n",
    "            print(f'ffmpeg not found in {ffmpeg_bin_dir}')\n",
    "        else:\n",
    "            print('ffmpeg installed')\n",
    "    else:\n",
    "        subprocess.check_call(['sudo', 'apt-get', 'update'])\n",
    "        subprocess.check_call(['sudo', 'apt-get', 'install', '-y', 'ffmpeg'])\n",
    "        print('ffmpeg installed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250f3b9-c4fd-42ad-aaea-581ef2f05e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup all lib and env\n",
    "\n",
    "# fastapi :\n",
    "try:\n",
    "    import fastapi                      \n",
    "    print('fastapi: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q fastapi\n",
    "  print('Installed fastapi')\n",
    "    \n",
    "# uvicorn :\n",
    "try:\n",
    "    import uvicorn                      \n",
    "    print('uvicorn: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q uvicorn\n",
    "  print('Installed uvicorn')\n",
    "    \n",
    "# transformers :\n",
    "try:\n",
    "    import transformers                      \n",
    "    print('transformers: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q transformers\n",
    "  print('Installed transformers')\n",
    "    \n",
    "# TODO torch, vision, audio and difusion model\n",
    "# torch :\n",
    "try:\n",
    "    import torch                      \n",
    "    print('torch: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q torch\n",
    "  print('Installed torch')\n",
    "    \n",
    "# torchvision :\n",
    "try:\n",
    "    import torchvision                      \n",
    "    print('torchvision: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q torchvision\n",
    "  print('Installed torchvision')\n",
    "    \n",
    "# ftfy :\n",
    "try:\n",
    "    import ftfy                      \n",
    "    print('ftfy: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q ftfy\n",
    "  print('Installed ftfy')\n",
    "\n",
    "# torchaudio :\n",
    "try:\n",
    "    import torchaudio                      \n",
    "    print('torchaudio: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q torchaudio\n",
    "  print('Installed torchaudio')\n",
    "    \n",
    "# python-multipart :\n",
    "try:\n",
    "    import python_multipart                      \n",
    "    print('python-multipart: already installed')\n",
    "except ImportError:\n",
    "    !python -m pip install -q python-multipart\n",
    "    print('Installed python-multipart')\n",
    "\n",
    "    \n",
    "# pip install diffusers[\"torch\"]\n",
    "\n",
    "# # sb3_diffusion :\n",
    "# try:\n",
    "#     import sb3_diffusion                      \n",
    "#     print('sb3_diffusion (stabl diffsuin 3): already installed')\n",
    "# except ImportError:\n",
    "#   !python -m pip install -q sb3_diffusion\n",
    "#   print('Installed sb3_diffusion(stabl diffsuin 3')\n",
    "    \n",
    "# webrtcvad :\n",
    "try:\n",
    "    import webrtcvad                     \n",
    "    print('webrtcvad: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q webrtcvad\n",
    "  print('Installed webrtcvad')\n",
    "    \n",
    "# redis :\n",
    "try:\n",
    "    import redis                      \n",
    "    print('redis: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q redis\n",
    "  print('Installed redis')\n",
    "    \n",
    "# aioredis :\n",
    "try:\n",
    "    import aioredis                      \n",
    "    print('aioredis: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q aioredis\n",
    "  print('Installed aioredis')\n",
    "    \n",
    "# python-socketio[client] :\n",
    "try:\n",
    "    import socketio                      \n",
    "    print('python-socketio[client]: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q python-socketio[client]\n",
    "  print('Installed socketio')\n",
    "    \n",
    "# aiortc :\n",
    "try:\n",
    "    import aiortc                      \n",
    "    print('aiortc: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q aiortc\n",
    "  print('Installed aiortc')\n",
    "    \n",
    "# av :\n",
    "try:\n",
    "    import av                      \n",
    "    print('av: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q av\n",
    "  print('Installed av')\n",
    "    \n",
    "# aiohttp :\n",
    "try:\n",
    "    import aiohttp                      \n",
    "    print('aiohttp: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q aiohttp\n",
    "  print('Installed aiohttp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930d40b-f87f-4d94-a00b-8ddff86f932e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Setup Language Trasnformer Stream API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57308397-be85-4ba9-a742-caac31b44dcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059a309-b101-4c04-8d75-0260c99fb04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import redis\n",
    "import requests\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import webrtcvad\n",
    "\n",
    "# FastAPI and related libraries\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from functools import lru_cache\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Transformers, GPT-2, BERT, and DALLE-mini\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, GPT2Tokenizer, \n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "# TODO\n",
    "# # CLIP\n",
    "# import clip\n",
    "\n",
    "# # Stable Baselines 3 diffusion\n",
    "# from sb3_diffusion import get_prompt_finetune_optimizer, create_diffusion_callback\n",
    "\n",
    "# aiortc and WebRTC\n",
    "from aiortc import RTCPeerConnection, RTCSessionDescription\n",
    "from aiortc.contrib.media import MediaPlayer, MediaRecorder, MediaStreamTrack\n",
    "import cv2\n",
    "\n",
    "#  TODO\n",
    "# # Load the best large-scale diffusion model and tokenizer\n",
    "# default_clip_model_name = \"lucidrains/big-sleep\"\n",
    "# clip_model = clip.load(default_clip_model_name).eval().cuda()\n",
    "# clip_processor = clip.tokenize\n",
    "\n",
    "# # Load the default CLIP model and processor\n",
    "# clip_model, clip_processor = clip.load(\"ViT-B/32\")\n",
    "\n",
    "# Load the default GPT-2 model and tokenizer\n",
    "gpt2_model_name = \"gpt2-large\"\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# TODO\n",
    "# # Load the default text-to-image model\n",
    "# diffuse_model_name = \"sb3-diffusion/512x512_diffusion_unconditional_imagenet_8s_256it.pt\"\n",
    "# diffuse_model = Diffusion(prompt_size=256, image_size=512, diffusion_steps=1000, denoise_scale=0.1)\n",
    "# \n",
    "# # Load ControlNet model\n",
    "# controlnet_model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512',\n",
    "#                        pretrained=True, useGPU=torch.cuda.is_available())\n",
    "# \n",
    "\n",
    "# Load the default text-to-speech model\n",
    "def load_tts_model(model_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.AutoModelWithLMHead.from_pretrained(model_name).to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "default_tts_model_name = \"tts_models/tts_model_mellotron_ljspeech.pt\"\n",
    "default_tts_model_name = \"ttskit/gpt2-ljspeech-melgan\"\n",
    "tts_tokenizer, tts_model = load_tts_model(default_tts_model_name)\n",
    "tts_vocoder = torchaudio.models.WaveGlow(n_mel_channels=80, n_flows=12, n_group=8, n_early_every=4, n_early_size=2, WN_config={'n_layers': 8, 'n_channels': 256}).cuda()\n",
    "\n",
    "# Set device for vocoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tts_vocoder = tts_vocoder.to(device)\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Redis configuration\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "redis_pubsub = redis_client.pubsub()\n",
    "\n",
    "# Socket.IO client\n",
    "sio = socketio.AsyncClient()\n",
    "\n",
    "# # Text-to-image input model\n",
    "# class TextToImageInput(BaseModel):\n",
    "#     image: Optional[UploadFile] = None\n",
    "#     text: Optional[str] = None\n",
    "#     model_name: str = diffuse_model\n",
    "#     temperature: float = 0.9\n",
    "#     top_p: float = 0.99\n",
    "#     max_length: int = 256\n",
    "#     seed: Optional[int] = None\n",
    "\n",
    "# # Image-to-text input model\n",
    "# class ImageToTextInput(BaseModel):\n",
    "#     image_url: str\n",
    "#     text: str\n",
    "#     model_name: str = clip_model.name_or_path\n",
    "\n",
    "# Text generation input model\n",
    "class TextGenerationInput(BaseModel):\n",
    "    text: str\n",
    "    model_name: str = gpt2_model_name\n",
    "    \n",
    "# Speech-to-text input model\n",
    "class SpeechToTextInput(BaseModel):\n",
    "    audio_file: UploadFile\n",
    "    vad_aggressiveness: int = 3\n",
    "    sample_rate: int = 16000\n",
    "\n",
    "# Speech-to-speech input model\n",
    "class SpeechToSpeechInput(BaseModel):\n",
    "    audio_file: UploadFile\n",
    "    source_language: str\n",
    "    target_language: str\n",
    "    \n",
    "# Text-to-speech input model\n",
    "class TextToSpeechInput(BaseModel):\n",
    "    text: str\n",
    "    voice_name: str = \"en-US-Wavenet-D\"\n",
    "\n",
    "# Initialize VAD (voice activity detection)\n",
    "vad = webrtcvad.Vad()\n",
    "\n",
    "# Constants for audio processing\n",
    "SILENCE_CHUNK_DURATION_MS = 500\n",
    "MAX_AUDIO_DURATION_S = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a471a-b628-4e42-94e6-ba5afe553758",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 . Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97fe25-e38f-42c0-9c86-d1b9aeb17b9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for text-to-text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de36398-2332-478b-9036-636ba7e595d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@app.post(\"/text-to-text\")\n",
    "async def text_to_text(text_generation_input: TextGenerationInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if text_generation_input.model_name != gpt2_model_name:\n",
    "        model = GPT2LMHeadModel.from_pretrained(text_generation_input.model_name)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(text_generation_input.model_name)\n",
    "    else:\n",
    "        model = gpt2_model\n",
    "        tokenizer = gpt2_tokenizer\n",
    "\n",
    "    # Generate text using the specified model and settings\n",
    "    prompt = text_generation_input.text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids=input_ids.cuda(),\n",
    "                            max_length=256,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(event_id, json.dumps({\"text\": prompt, \"generated_text\": generated_text}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-text_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ed37d-bf48-4020-aee7-5c0cb911b10b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Image Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68430f3c-449d-41c9-9e3b-5d833e9d167f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for text-to-image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9c1ab-6d77-4560-a305-ad093ddd308e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FastAPI endpoint for text-to-image generation\n",
    "@app.post(\"/text-to-image\")\n",
    "async def text_to_image(text_to_image_input: TextToImageInput):\n",
    "    # Generate a unique ID for the event\n",
    "    event_id = str(uuid.uuid4())\n",
    "\n",
    "    # If an image was uploaded, use it, otherwise use the provided text\n",
    "    if text_to_image_input.image is not None:\n",
    "        # Load the image into memory\n",
    "        image_bytes = await text_to_image_input.image.read()\n",
    "        image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        image_tensor = T.ToTensor()(image).unsqueeze(0).cuda()\n",
    "\n",
    "        # Use the text-to-image model to generate an image from the provided text\n",
    "        if text_to_image_input.text is not None:\n",
    "            prompt = f\"{text_to_image_input.text} | image prompt\"\n",
    "        else:\n",
    "            prompt = \"image prompt\"\n",
    "        \n",
    "        # Generate the image using the specified model and settings\n",
    "        output = model.generate(\n",
    "            **tokenizer(prompt, return_tensors=\"pt\").to(image_tensor.device),\n",
    "            do_sample=True,\n",
    "            temperature=text_to_image_input.temperature,\n",
    "            top_p=text_to_image_input.top_p,\n",
    "            max_length=text_to_image_input.max_length,\n",
    "            seed=text_to_image_input.seed,\n",
    "        )\n",
    "        generated_image = T.ToPILImage()(output[0].cpu().clamp(0, 1))\n",
    "    else:\n",
    "        # Generate an image from the provided text\n",
    "        prompt = text_to_image_input.text\n",
    "        \n",
    "        # Generate the image using the specified model and settings\n",
    "        output = model.generate(\n",
    "            **tokenizer(prompt, return_tensors=\"pt\").to(model.device),\n",
    "            do_sample=True,\n",
    "            temperature=text_to_image_input.temperature,\n",
    "            top_p=text_to_image_input.top_p,\n",
    "            max_length=text_to_image_input.max_length,\n",
    "            seed=text_to_image_input.seed,\n",
    "        )\n",
    "        generated_image = T.ToPILImage()(output[0].cpu().clamp(0, 1))\n",
    "\n",
    "    # Save the generated image and text to Redis\n",
    "    if text_to_image_input.text is not None:\n",
    "        redis_client.set(event_id, json.dumps({\"image\": generated_image, \"text\": text_to_image_input.text}))\n",
    "    else:\n",
    "        redis_client.set(event_id, json.dumps({\"image\": generated_image}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-image_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b5b98-d201-4160-97d0-2b909bb5d228",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for image-to-text processing using CLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5295d0-66f1-41db-94a4-9f2fcc63392e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@app.post(\"/image-to-text\")\n",
    "    async def image_to_text(image_to_text_input: ImageToTextInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if image_to_text_input.model_name != default_clip_model_name:\n",
    "        clip_model = clip.load(image_to_text_input.model_name).eval().cuda()\n",
    "    else:\n",
    "        clip_model = clip.load(default_clip_model_name).eval().cuda()\n",
    "\n",
    "    # Download the image\n",
    "    response = requests.get(image_to_text_input.image_url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    # Process the image using the CLIP model\n",
    "    input_image = T.ToTensor()(image).unsqueeze(0).cuda()\n",
    "    input_text = clip_processor(image_to_text_input.text).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        logits_per_image, logits_per_text = clip_model(input_image, input_text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(event_id, json.dumps({\"image_url\": image_to_text_input.image_url, \"text\": str(probs[0])}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"image-to-text_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd319d-ae79-4984-a2b4-c99bf7d482b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb1794-5afd-4e77-81af-8076f1f6af37",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## FastAPI endpoint for text-to-image generation using diffusion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6c1b5-8b54-4e44-9453-1ac09954140e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FastAPI endpoint for text-to-image generation using diffusion models\n",
    "@app.post(\"/text-to-image-diffuse\")\n",
    "async def text_to_image_diffuse(text_to_image_input: TextToImageInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if text_to_image_input.model_name != diffuse_model_name:\n",
    "        diffuse_model = Diffusion(prompt_size=256, image_size=512, diffusion_steps=text_to_image_input.diffusion_steps, denoise_scale=text_to_image_input.denoise_scale)\n",
    "    else:\n",
    "        diffuse_model = Diffusion(prompt_size=256, image_size=512, diffusion_steps=1000, denoise_scale=0.1)\n",
    "\n",
    "    # Generate unique ID for the event\n",
    "    event_id = str(uuid.uuid4())\n",
    "\n",
    "    if text_to_image_input.text is not None:\n",
    "        # Generate an image from the provided text\n",
    "        prompt = text_to_image_input.text\n",
    "\n",
    "        # Generate the image using the specified model and settings\n",
    "        output = diffuse_model.sample(\n",
    "            text=prompt,\n",
    "            clip_model=clip_model,\n",
    "            diffusion_steps=text_to_image_input.diffusion_steps,\n",
    "            temperature=text_to_image_input.temperature,\n",
    "            clip_guidance_scale=text_to_image_input.clip_guidance_scale,\n",
    "            tv_scale=text_to_image_input.tv_scale,\n",
    "            range_scale=text_to_image_input.range_scale,\n",
    "        )\n",
    "        generated_image = Image.fromarray((255 * output.permute(0, 2, 3, 1).cpu().numpy()).astype(np.uint8)[0])\n",
    "\n",
    "        # Save the results to Redis\n",
    "        image_bytes = BytesIO()\n",
    "        generated_image.save(image_bytes, format=\"PNG\")\n",
    "        redis_client.set(event_id, json.dumps({\"text\": prompt, \"image\": base64.b64encode(image_bytes.getvalue()).decode(\"utf-8\")}))\n",
    "    else:\n",
    "        # Generate text using the specified model and settings\n",
    "        prompt = text_to_image_input.image_caption\n",
    "        if prompt is None:\n",
    "            prompt = \"An image generated using text-to-image processing.\"\n",
    "\n",
    "        # Generate the image using the specified model and settings\n",
    "        output = diffuse_model.sample_text(prompt, text_len=text_to_image_input.text_length)\n",
    "        generated_text = output[0][\"text\"]\n",
    "\n",
    "        # Save the results to Redis\n",
    "        redis_client.set(event_id, json.dumps({\"image_caption\": prompt, \"generated_text\": generated_text}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-image-diffuse_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88880176-ba06-40fd-973c-b50f7ae7c898",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## FastAPI endpoint for image-to-image generation using controlnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60505c-236f-479e-8749-224147859d22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FastAPI endpoint for image manipulation using ControlNet\n",
    "@app.post(\"/image-manipulation\")\n",
    "async def image_manipulation(image_manipulation_input: ImageManipulationInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if image_manipulation_input.model_name != default_controlnet_model_name:\n",
    "        controlnet_model, controlnet_tokenizer = load_controlnet_model(image_manipulation_input.model_name)\n",
    "    else:\n",
    "        controlnet_model, controlnet_tokenizer = load_controlnet_model(default_controlnet_model_name)\n",
    "\n",
    "    # Download the image\n",
    "    response = requests.get(image_manipulation_input.image_url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    # Load the image into memory and encode it as base64\n",
    "    image_bytes = BytesIO()\n",
    "    image.save(image_bytes, format=\"JPEG\")\n",
    "    image_data = base64.b64encode(image_bytes.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Generate a list of attribute-value pairs from the provided text\n",
    "    attribute_values = []\n",
    "    for attribute_value in image_manipulation_input.attribute_values:\n",
    "        attribute, value = attribute_value.split(\":\")\n",
    "        attribute_values.append((attribute.strip(), value.strip()))\n",
    "\n",
    "    # Generate a new image using ControlNet\n",
    "    output_image = generate_image_from_attributes(\n",
    "        controlnet_model,\n",
    "        controlnet_tokenizer,\n",
    "        attribute_values,\n",
    "        source_image=image,\n",
    "        max_num_iterations=image_manipulation_input.max_num_iterations,\n",
    "        save_interval=image_manipulation_input.save_interval,\n",
    "        save_dir=\"./generated_images\",\n",
    "    )\n",
    "\n",
    "    # Load the generated image into memory and encode it as base64\n",
    "    output_bytes = BytesIO()\n",
    "    output_image.save(output_bytes, format=\"JPEG\")\n",
    "    output_data = base64.b64encode(output_bytes.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(\n",
    "        event_id,\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"image_url\": image_manipulation_input.image_url,\n",
    "                \"attribute_values\": image_manipulation_input.attribute_values,\n",
    "                \"output_image\": output_data,\n",
    "                \"input_image\": image_data,\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"image-manipulation_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62bf302-68a4-4f8d-86be-d2acef810640",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Audio Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d15195-8f7a-4aef-bc99-92385a14658b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for text-to-speech conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011741b-2a7b-419c-b4e7-5bea9b81f50f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FastAPI endpoint for text-to-speech conversion\n",
    "@app.post(\"/text-to-speech\")\n",
    "async def text_to_speech(text_to_speech_input: TextToSpeechInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if text_to_speech_input.model_name != default_tts_model_name:\n",
    "        tts_model, tts_parser = load_tts_model(text_to_speech_input.model_name)\n",
    "    else:\n",
    "        tts_model, tts_parser = load_tts_model(default_tts_model_name)\n",
    "\n",
    "    # Synthesize speech from the provided text\n",
    "    with torch.no_grad():\n",
    "        # Get the phonemes from the text using the text-to-phoneme model\n",
    "        phonemes = g2p(text_to_speech_input.text)\n",
    "\n",
    "        # Convert the phonemes to a tensor\n",
    "        input_ids = torch.LongTensor(tts_parser.text_to_sequence(phonemes)).unsqueeze(0).cuda()\n",
    "\n",
    "        # Synthesize speech using the text-to-speech model\n",
    "        audio = tts_model.inference(input_ids)\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    audio_bytes = audio.cpu().numpy().tobytes()\n",
    "    redis_client.set(event_id, json.dumps({\"text\": text_to_speech_input.text, \"audio\": base64.b64encode(audio_bytes).decode(\"utf-8\")}))\n",
    "    \n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-speech_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf78b4-d23b-4013-8d34-3302bfcd0f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for speech-to-text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf693c-85e4-48f7-8d90-d976e395f3f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FastAPI endpoint for speech-to-text conversion\n",
    "@app.post(\"/speech-to-text\")\n",
    "async def speech_to_text(audio_file: UploadFile, vad_aggressiveness: int = 3, sample_rate: int = 16000):\n",
    "    # Read the uploaded audio file into memory\n",
    "    audio_bytes = await audio_file.read()\n",
    "    audio_io = BytesIO(audio_bytes)\n",
    "\n",
    "    # Open the audio stream using PyTorch\n",
    "    with torch.no_grad():\n",
    "        # Load the audio waveform from the stream\n",
    "        waveform, sr = torchaudio.load(audio_io)\n",
    "\n",
    "        # Resample the waveform if necessary\n",
    "        if sr != sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert the waveform to mono\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Convert the waveform to the desired format for VAD\n",
    "        vad_input = (waveform * 32768).squeeze().numpy().astype(np.int16)\n",
    "\n",
    "        # Run VAD on the audio to detect speech segments\n",
    "        vad.set_mode(vad_aggressiveness)\n",
    "        frames = np.array_split(vad_input, len(vad_input) // (SILENCE_CHUNK_DURATION_MS * sample_rate // 1000))\n",
    "        speech_segments = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            is_speech = vad.is_speech(frame.tobytes(), sample_rate)\n",
    "            if is_speech:\n",
    "                speech_segments.append(i)\n",
    "\n",
    "        # If no speech was detected, return an empty string\n",
    "        if not speech_segments:\n",
    "            return {\"text\": \"\"}\n",
    "\n",
    "        # Extract the speech segments from the audio\n",
    "        speech_audio = torch.cat([waveform[:, segment * (SILENCE_CHUNK_DURATION_MS * sample_rate // 1000):(segment + 1) * (SILENCE_CHUNK_DURATION_MS * sample_rate // 1000)] for segment in speech_segments], dim=-1)\n",
    "\n",
    "        # Transcribe the speech to text using the specified model\n",
    "        transcribed_text = model(speech_audio, sample_rate=sample_rate)\n",
    "\n",
    "    # Generate a unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(event_id, json.dumps({\"audio\": base64.b64encode(audio_bytes).decode(\"utf-8\"), \"text\": transcribed_text}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"speech-to-text_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id, \"text\": transcribed_text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc1efd-4c24-463a-9249-583bcf5eb2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for speech-to-speech conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec034d-fd6a-4bdc-b28f-af4a0d9f4e9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# FastAPI endpoint for speech-to-speech conversion\n",
    "@app.post(\"/speech-to-speech\")\n",
    "async def speech_to_speech(speech_to_speech_input: SpeechToSpeechInput):\n",
    "    # Load the specified models or use the default ones\n",
    "    if speech_to_speech_input.source_language != \"en\":\n",
    "        stt_model, stt_parser = load_stt_model(speech_to_speech_input.source_language)\n",
    "    else:\n",
    "        stt_model, stt_parser = load_stt_model(default_stt_model_name)\n",
    "\n",
    "    if speech_to_speech_input.target_language != \"en\":\n",
    "        tts_model, tts_parser = load_tts_model(speech_to_speech_input.target_language)\n",
    "    else:\n",
    "        tts_model, tts_parser = load_tts_model(default_tts_model_name)\n",
    "\n",
    "    # Transcribe the audio from the source language\n",
    "    audio_bytes = await speech_to_speech_input.audio_file.read()\n",
    "    audio_tensor, _ = torchaudio.load(BytesIO(audio_bytes))\n",
    "    if audio_tensor.size(0) > 1:\n",
    "        audio_tensor = audio_tensor.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample the audio if necessary\n",
    "    if audio_tensor.shape[1] != stt_parser.sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(audio_tensor.shape[1], stt_parser.sample_rate)\n",
    "        audio_tensor = resampler(audio_tensor)\n",
    "\n",
    "    # Detect speech segments\n",
    "    speech_segments = detect_speech(audio_tensor, stt_parser.sample_rate)\n",
    "\n",
    "    # Transcribe the speech segments\n",
    "    transcriptions = []\n",
    "    for segment in speech_segments:\n",
    "        segment_tensor = audio_tensor[:, segment[0]:segment[1]]\n",
    "        with torch.no_grad():\n",
    "            input_signal = stt_parser.preprocess(segment_tensor)\n",
    "            input_signal = input_signal.cuda()\n",
    "            _, predicted = stt_model(input_signal, input_signal.new_zeros([input_signal.shape[0], 1], dtype=torch.long))\n",
    "            transcription = stt_parser.decode(predicted[0])\n",
    "            transcriptions.append(transcription)\n",
    "\n",
    "    # Translate the transcriptions to the target language\n",
    "    translations = []\n",
    "    for transcription in transcriptions:\n",
    "        translation = translator.translate(transcription, src=speech_to_speech_input.source_language, dest=speech_to_speech_input.target_language)\n",
    "        translations.append(translation.text)\n",
    "\n",
    "    # Synthesize speech in the target language\n",
    "    with torch.no_grad():\n",
    "        # Get the phonemes from the translated text using the text-to-phoneme model\n",
    "        phonemes = g2p(translations)\n",
    "\n",
    "        # Convert the phonemes to a tensor\n",
    "        input_ids = torch.LongTensor(tts_parser.text_to_sequence(phonemes)).unsqueeze(0).cuda()\n",
    "\n",
    "        # Synthesize speech using the text-to-speech model\n",
    "        audio = tts_model.inference(input_ids)\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    audio_bytes = audio.cpu().numpy().tobytes()\n",
    "    redis_client.set(event_id, json.dumps({\"audio\": base64.b64encode(audio_bytes).decode(\"utf-8\")}))\n",
    "    redis_client.publish(\"speech-to-speech_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f88d0-41c0-4939-9ff7-150ddda2a34c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Match Moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c111040-c243-4fb5-8a86-984f07ea5f31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ebc2e8-3e3d-4776-a8ee-820ba4f88276",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Run the FastAPI application in the Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4c362-3911-4347-9fd2-da82d9e2b6b9",
   "metadata": {},
   "source": [
    "> Note that the pyngrok package is optional and only needed if you want to expose your API to the internet using ngrok. To install the pyngrok package, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be202e-f5f4-4bb9-99cc-7530028f9983",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install pyngrok\n",
    "# pyngrok :\n",
    "try:\n",
    "    import pyngrok                      \n",
    "    print('pyngrok: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q pyngrok\n",
    "  print('Installed pyngrok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846934e4-3146-4bd8-9f14-61f9e78ad549",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up ngrok for external access (optional)\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
    "\n",
    "# Run the FastAPI app\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f5336-a431-41e6-a48f-4d602b98d065",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec63b8e-0f38-4d6d-ad1b-1f3542307185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
