{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da46986-7ab6-4812-a909-13b276bb5df5",
   "metadata": {},
   "source": [
    "# Virtual Large Language Transformer Micro Server\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d295c-93cf-42c4-9f2a-3c9737b53024",
   "metadata": {},
   "source": [
    "##  Setup all lib and env:\n",
    "**Python Lib:**\n",
    ">* FastApi - *for client access* \n",
    ">* redis - *for base data*\n",
    ">* websockets - *for desktop control* \n",
    ">* aiortc - *for desktop streaming*\n",
    "\n",
    "```\n",
    "pip install fastapi\n",
    "pip install uvicorn\n",
    "pip install transformers\n",
    "pip install torch\n",
    "pip install redis\n",
    "pip install aioredis\n",
    "pip install python-socketio[client]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b42302-da78-43ea-aeee-476e38d532e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastapi: already installed\n",
      "uvicorn: already installed\n",
      "transformers: already installed\n",
      "torch: already installed\n",
      "redis: already installed\n",
      "aioredis: already installed\n",
      "python-socketio[client]: already installed\n"
     ]
    }
   ],
   "source": [
    "# Setup all lib and env\n",
    "\n",
    "# fastapi :\n",
    "try:\n",
    "    import fastapi                      \n",
    "    print('fastapi: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q fastapi\n",
    "  print('Installed fastapi')\n",
    "    \n",
    "# uvicorn :\n",
    "try:\n",
    "    import uvicorn                      \n",
    "    print('uvicorn: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q uvicorn\n",
    "  print('Installed uvicorn')\n",
    "    \n",
    "# transformers :\n",
    "try:\n",
    "    import transformers                      \n",
    "    print('transformers: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q transformers\n",
    "  print('Installed transformers')\n",
    "    \n",
    "# torch :\n",
    "try:\n",
    "    import torch                      \n",
    "    print('torch: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q torch\n",
    "  print('Installed torch')\n",
    "    \n",
    "# redis :\n",
    "try:\n",
    "    import redis                      \n",
    "    print('redis: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q redis\n",
    "  print('Installed redis')\n",
    "    \n",
    "# aioredis :\n",
    "try:\n",
    "    import aioredis                      \n",
    "    print('aioredis: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q aioredis\n",
    "  print('Installed aioredis')\n",
    "    \n",
    "# python-socketio[client] :\n",
    "try:\n",
    "    import socketio                      \n",
    "    print('python-socketio[client]: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q python-socketio[client]\n",
    "  print('Installed socketio')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818d692-cc8e-44f2-b92f-7e69265ce3ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Language Trasnformer Stream API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2de3b3-956e-4c0f-b47c-8b5a12e3efe3",
   "metadata": {},
   "source": [
    "## Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9f2f05-ae6e-437b-8e70-ad9f877e8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from fastapi import FastAPI, WebSocket\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pydantic import BaseModel\n",
    "import redis\n",
    "import aioredis\n",
    "import socketio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e815c-d899-4001-be3b-83f89338cc3e",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the FastAPI app, Redis configuration, and Socket.IO client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a27bdc-214f-4bd0-b9b0-40a9f744d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "# Redis configuration\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "redis_pubsub = redis_client.pubsub()\n",
    "\n",
    "# Socket.IO client\n",
    "sio = socketio.AsyncClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881252ae-c2b8-423c-8b75-f1997d1808d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the pre-trained model and tokenizer:\n",
    "**Choose a pre-trained model:**\n",
    "\n",
    "Select a suitable pre-trained model from the Hugging Face Model Hub [Hugginface/models](https://huggingface.co/models). \n",
    "> For example, you can use \"gpt2\" or \"gpt2-medium\" if you want a model similar to GPT-3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c5c1d-355f-4b74-a1e9-0679ed10e91e",
   "metadata": {},
   "source": [
    "## Prepare your dataset:\n",
    "\n",
    "> Transform your dataset into a format suitable for training. You'll need to tokenize your text data and create input sequences. You can use the GPT2Tokenizer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfa8ab0-e880-4428-8df8-d84bb1b6e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ddb8b-53d3-47c0-a87c-4e6d8c3d6cc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fine-tune the model:\n",
    "\n",
    ">Fine-tune the pre-trained model on your dataset using a suitable training loop. You can use the Trainer class from the\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8fee0-f3d2-45bf-874d-fd16b20a31c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create a Pydantic model to accept input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c2514d-68f1-40c3-802f-ded64c1b785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationInput(BaseModel):\n",
    "    text: str\n",
    "    model_name: str = \"gpt2-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8cbcc-11ea-460f-82a8-6b442321bac9",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the FastAPI endpoint for text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e20ace6-cf65-4fff-83da-f106bad67fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/generate\")\n",
    "async def generate_text(input_data: TextGenerationInput):\n",
    "    model_name = input_data.model_name\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    input_text = input_data.text\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output_tokens = model.generate(input_tokens)\n",
    "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    # Log interaction to Redis\n",
    "    interaction_data = {\n",
    "        \"input\": input_text,\n",
    "        \"output\": output_text,\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "    redis_client.set(\"interaction\", json.dumps(interaction_data))\n",
    "\n",
    "    # Publish event to Redis Pub/Sub\n",
    "    redis_client.publish(\"events\", json.dumps(interaction_data))\n",
    "\n",
    "    return {\"generated_text\": output_text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdb2ad-8b2d-438b-8f64-1af7b6390eca",
   "metadata": {},
   "source": [
    "---\n",
    "## Set up WebSocket for real-time communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84763028-9aa2-4ddf-a005-a330836038b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.websocket(\"/ws\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "\n",
    "    while True:\n",
    "        data = await websocket.receive_text()\n",
    "        input_data = TextGenerationInput(**json.loads(data))\n",
    "        model_name = input_data.model_name\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "        input_text = input_data.text\n",
    "        input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        output_tokens = model.generate(input_tokens)\n",
    "        output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Log interaction to Redis\n",
    "        interaction_data = {\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text,\n",
    "            \"model_name\": model_name,\n",
    "        }\n",
    "        redis_client.set(\"interaction\", json.dumps(interaction_data))\n",
    "\n",
    "        # Publish event to Redis Pub/Sub\n",
    "        redis_client.publish(\"events\", json.dumps(interaction_data))\n",
    "\n",
    "        # Send generated text via WebSocket\n",
    "        await websocket.send_text(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde099fd-2e11-4267-8cfe-981f44716ecd",
   "metadata": {},
   "source": [
    "> Note that the pyngrok package is optional and only needed if you want to expose your API to the internet using ngrok. To install the pyngrok package, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c02add-6b0d-429c-bbf8-19f77f9908c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyngrok: already installed\n"
     ]
    }
   ],
   "source": [
    "#pip install pyngrok\n",
    "# pyngrok :\n",
    "try:\n",
    "    import pyngrok                      \n",
    "    print('pyngrok: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q pyngrok\n",
    "  print('Installed pyngrok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897aba15-5539-45a7-abdc-5804f6595e72",
   "metadata": {},
   "source": [
    "---\n",
    "## Run the FastAPI application in the Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a0a07c-ad8d-4573-8127-333d96eafa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up ngrok for external access (optional)\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
    "\n",
    "# Run the FastAPI app\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada0446-d332-4649-9657-690fef967472",
   "metadata": {},
   "source": [
    "---\n",
    "## Test API with a gardio interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa375aa5-8e1d-4b3c-add7-5fe90c4da0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradio: already installed\n"
     ]
    }
   ],
   "source": [
    "# # pip install gradio\n",
    "\n",
    "# # gradio :\n",
    "# try:\n",
    "#     import gradio                      \n",
    "#     print('gradio: already installed')\n",
    "# except ImportError:\n",
    "#   !python -m pip install -q gradio\n",
    "#   print('Installed gradio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6221e4-4739-416c-85e1-e68e0c6e187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# def generate_text(input_text):\n",
    "#     import requests\n",
    "#     response = requests.post(\"http://localhost:8000/generate\", json={\"text\": input_text})\n",
    "#     return response.json()[\"generated_text\"]\n",
    "\n",
    "# iface = gr.Interface(fn=generate_text, inputs=\"text\", outputs=\"text\", title=\"GPT-2 Text Generation\")\n",
    "# iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52210eb3-e446-452e-95b0-75489d677b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
