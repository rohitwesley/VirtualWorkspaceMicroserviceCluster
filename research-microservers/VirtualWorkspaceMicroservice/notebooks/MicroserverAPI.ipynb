{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15af77e0-203a-4b0e-9cd5-ff4cc9698046",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Virtual Workspace Web App Microserver API\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4aa1ca-e66f-4f36-94ca-362657b2deed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  Setup all lib and env:\n",
    "**Python Lib:**\n",
    ">* FastApi - *for client access* \n",
    ">* redis - *for base data*\n",
    ">* websockets - *for desktop control* \n",
    ">* aiortc - *for desktop streaming*\n",
    "\n",
    "```\n",
    "pip install fastapi\n",
    "pip install uvicorn\n",
    "pip install transformers\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "pip install sb3-contrib\n",
    "pip install redis\n",
    "pip install aioredis\n",
    "pip install python-socketio[client]\n",
    "pip install aiortc\n",
    "pip install av\n",
    "pip install aiohttp\n",
    "pip install transformers torch torchvision clip stable-baselines3 webrtcvad torchaudio\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10917ad7-1535-4185-bb00-a22065fdfd6c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg already installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if ffmpeg is installed\n",
    "try:\n",
    "    if os.name == 'nt':\n",
    "        subprocess.check_output(['where', 'ffmpeg'])\n",
    "    else:\n",
    "        subprocess.check_output(['ffmpeg', '-version'])\n",
    "    print('ffmpeg already installed')\n",
    "except subprocess.CalledProcessError:\n",
    "    # Install ffmpeg\n",
    "    print('ffmpeg not found. Installing...')\n",
    "    if os.name == 'nt':\n",
    "        ffmpeg_bin_dir = r'C:\\ProgramData\\chocolatey\\lib\\ffmpeg\\tools'\n",
    "        os.environ['PATH'] += os.pathsep + ffmpeg_bin_dir\n",
    "        if not os.path.exists(ffmpeg_bin_dir):\n",
    "            print(f'ffmpeg not found in {ffmpeg_bin_dir}')\n",
    "        else:\n",
    "            print('ffmpeg installed')\n",
    "    else:\n",
    "        subprocess.check_call(['sudo', 'apt-get', 'update'])\n",
    "        subprocess.check_call(['sudo', 'apt-get', 'install', '-y', 'ffmpeg'])\n",
    "        print('ffmpeg installed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d250f3b9-c4fd-42ad-aaea-581ef2f05e0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastapi: already installed\n",
      "uvicorn: already installed\n",
      "transformers: already installed\n",
      "torch: already installed\n",
      "torchvision: already installed\n",
      "ftfy: already installed\n",
      "torchaudio: already installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wesle\\anaconda3\\envs\\Microserver\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed python-multipart\n",
      "webrtcvad: already installed\n",
      "redis: already installed\n",
      "aioredis: already installed\n",
      "python-socketio[client]: already installed\n",
      "aiortc: already installed\n",
      "av: already installed\n",
      "aiohttp: already installed\n"
     ]
    }
   ],
   "source": [
    "# Setup all lib and env\n",
    "\n",
    "# fastapi :\n",
    "try:\n",
    "    import fastapi                      \n",
    "    print('fastapi: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q fastapi\n",
    "  print('Installed fastapi')\n",
    "    \n",
    "# uvicorn :\n",
    "try:\n",
    "    import uvicorn                      \n",
    "    print('uvicorn: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q uvicorn\n",
    "  print('Installed uvicorn')\n",
    "    \n",
    "# transformers :\n",
    "try:\n",
    "    import transformers                      \n",
    "    print('transformers: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q transformers\n",
    "  print('Installed transformers')\n",
    "    \n",
    "# TODO torch, vision, audio and difusion model\n",
    "# torch :\n",
    "try:\n",
    "    import torch                      \n",
    "    print('torch: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q torch\n",
    "  print('Installed torch')\n",
    "    \n",
    "# torchvision :\n",
    "try:\n",
    "    import torchvision                      \n",
    "    print('torchvision: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q torchvision\n",
    "  print('Installed torchvision')\n",
    "    \n",
    "# ftfy :\n",
    "try:\n",
    "    import ftfy                      \n",
    "    print('ftfy: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q ftfy\n",
    "  print('Installed ftfy')\n",
    "\n",
    "# torchaudio :\n",
    "try:\n",
    "    import torchaudio                      \n",
    "    print('torchaudio: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q torchaudio\n",
    "  print('Installed torchaudio')\n",
    "    \n",
    "# python-multipart :\n",
    "try:\n",
    "    import python_multipart                      \n",
    "    print('python-multipart: already installed')\n",
    "except ImportError:\n",
    "    !python -m pip install -q python-multipart\n",
    "    print('Installed python-multipart')\n",
    "\n",
    "    \n",
    "# pip install diffusers[\"torch\"]\n",
    "\n",
    "# # sb3_diffusion :\n",
    "# try:\n",
    "#     import sb3_diffusion                      \n",
    "#     print('sb3_diffusion (stabl diffsuin 3): already installed')\n",
    "# except ImportError:\n",
    "#   !python -m pip install -q sb3_diffusion\n",
    "#   print('Installed sb3_diffusion(stabl diffsuin 3')\n",
    "    \n",
    "# webrtcvad :\n",
    "try:\n",
    "    import webrtcvad                     \n",
    "    print('webrtcvad: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q webrtcvad\n",
    "  print('Installed webrtcvad')\n",
    "    \n",
    "# redis :\n",
    "try:\n",
    "    import redis                      \n",
    "    print('redis: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q redis\n",
    "  print('Installed redis')\n",
    "    \n",
    "# aioredis :\n",
    "try:\n",
    "    import aioredis                      \n",
    "    print('aioredis: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q aioredis\n",
    "  print('Installed aioredis')\n",
    "    \n",
    "# python-socketio[client] :\n",
    "try:\n",
    "    import socketio                      \n",
    "    print('python-socketio[client]: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q python-socketio[client]\n",
    "  print('Installed socketio')\n",
    "    \n",
    "# aiortc :\n",
    "try:\n",
    "    import aiortc                      \n",
    "    print('aiortc: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q aiortc\n",
    "  print('Installed aiortc')\n",
    "    \n",
    "# av :\n",
    "try:\n",
    "    import av                      \n",
    "    print('av: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q av\n",
    "  print('Installed av')\n",
    "    \n",
    "# aiohttp :\n",
    "try:\n",
    "    import aiohttp                      \n",
    "    print('aiohttp: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q aiohttp\n",
    "  print('Installed aiohttp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930d40b-f87f-4d94-a00b-8ddff86f932e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Setup Language Trasnformer Stream API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57308397-be85-4ba9-a742-caac31b44dcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5059a309-b101-4c04-8d75-0260c99fb04a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ttskit/gpt2-ljspeech-melgan is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/ttskit/gpt2-ljspeech-melgan/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\huggingface_hub\\file_download.py:1160\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1160\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\huggingface_hub\\file_download.py:1501\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1492\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1493\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1494\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1499\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:291\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    283\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m     )\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-642c6cdb-25d8369e4b51b0864c1668e1)\n\nRepository Not Found for url: https://huggingface.co/ttskit/gpt2-ljspeech-melgan/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m default_tts_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts_models/tts_model_mellotron_ljspeech.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m default_tts_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mttskit/gpt2-ljspeech-melgan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 77\u001b[0m tts_tokenizer, tts_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_tts_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_tts_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m tts_vocoder \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mWaveGlow(n_mel_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, n_flows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, n_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, n_early_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, n_early_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, WN_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_channels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m256\u001b[39m})\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Set device for vocoder\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 71\u001b[0m, in \u001b[0;36mload_tts_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tts_model\u001b[39m(model_name):\n\u001b[0;32m     70\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoModelWithLMHead\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer, model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:619\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    621\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:463\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    462\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 463\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: ttskit/gpt2-ljspeech-melgan is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import redis\n",
    "import requests\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import webrtcvad\n",
    "\n",
    "# FastAPI and related libraries\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from functools import lru_cache\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Transformers, GPT-2, BERT, and DALLE-mini\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, GPT2Tokenizer, \n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "# TODO\n",
    "# # CLIP\n",
    "# import clip\n",
    "\n",
    "# # Stable Baselines 3 diffusion\n",
    "# from sb3_diffusion import get_prompt_finetune_optimizer, create_diffusion_callback\n",
    "\n",
    "# aiortc and WebRTC\n",
    "from aiortc import RTCPeerConnection, RTCSessionDescription\n",
    "from aiortc.contrib.media import MediaPlayer, MediaRecorder, MediaStreamTrack\n",
    "import cv2\n",
    "\n",
    "#  TODO\n",
    "# # Load the best large-scale diffusion model and tokenizer\n",
    "# default_clip_model_name = \"lucidrains/big-sleep\"\n",
    "# clip_model = clip.load(default_clip_model_name).eval().cuda()\n",
    "# clip_processor = clip.tokenize\n",
    "\n",
    "# # Load the default CLIP model and processor\n",
    "# clip_model, clip_processor = clip.load(\"ViT-B/32\")\n",
    "\n",
    "# Load the default GPT-2 model and tokenizer\n",
    "gpt2_model_name = \"gpt2-large\"\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# TODO\n",
    "# # Load the default text-to-image model\n",
    "# diffuse_model_name = \"sb3-diffusion/512x512_diffusion_unconditional_imagenet_8s_256it.pt\"\n",
    "# diffuse_model = Diffusion(prompt_size=256, image_size=512, diffusion_steps=1000, denoise_scale=0.1)\n",
    "# \n",
    "# # Load ControlNet model\n",
    "# controlnet_model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512',\n",
    "#                        pretrained=True, useGPU=torch.cuda.is_available())\n",
    "# \n",
    "\n",
    "# Load the default text-to-speech model\n",
    "def load_tts_model(model_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.AutoModelWithLMHead.from_pretrained(model_name).to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "default_tts_model_name = \"tts_models/tts_model_mellotron_ljspeech.pt\"\n",
    "default_tts_model_name = \"ttskit/gpt2-ljspeech-melgan\"\n",
    "tts_tokenizer, tts_model = load_tts_model(default_tts_model_name)\n",
    "tts_vocoder = torchaudio.models.WaveGlow(n_mel_channels=80, n_flows=12, n_group=8, n_early_every=4, n_early_size=2, WN_config={'n_layers': 8, 'n_channels': 256}).cuda()\n",
    "\n",
    "# Set device for vocoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tts_vocoder = tts_vocoder.to(device)\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Redis configuration\n",
    "REDIS_URL = \"redis://localhost:6379\"\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "redis_pubsub = redis_client.pubsub()\n",
    "\n",
    "# Socket.IO client\n",
    "sio = socketio.AsyncClient()\n",
    "\n",
    "# # Text-to-image input model\n",
    "# class TextToImageInput(BaseModel):\n",
    "#     image: Optional[UploadFile] = None\n",
    "#     text: Optional[str] = None\n",
    "#     model_name: str = diffuse_model\n",
    "#     temperature: float = 0.9\n",
    "#     top_p: float = 0.99\n",
    "#     max_length: int = 256\n",
    "#     seed: Optional[int] = None\n",
    "\n",
    "# # Image-to-text input model\n",
    "# class ImageToTextInput(BaseModel):\n",
    "#     image_url: str\n",
    "#     text: str\n",
    "#     model_name: str = clip_model.name_or_path\n",
    "\n",
    "# Text generation input model\n",
    "class TextGenerationInput(BaseModel):\n",
    "    text: str\n",
    "    model_name: str = gpt2_model_name\n",
    "    \n",
    "# Speech-to-text input model\n",
    "class SpeechToTextInput(BaseModel):\n",
    "    audio_file: UploadFile\n",
    "    vad_aggressiveness: int = 3\n",
    "    sample_rate: int = 16000\n",
    "\n",
    "# Speech-to-speech input model\n",
    "class SpeechToSpeechInput(BaseModel):\n",
    "    audio_file: UploadFile\n",
    "    source_language: str\n",
    "    target_language: str\n",
    "    \n",
    "# Text-to-speech input model\n",
    "class TextToSpeechInput(BaseModel):\n",
    "    text: str\n",
    "    voice_name: str = \"en-US-Wavenet-D\"\n",
    "\n",
    "# Initialize VAD (voice activity detection)\n",
    "vad = webrtcvad.Vad()\n",
    "\n",
    "# Constants for audio processing\n",
    "SILENCE_CHUNK_DURATION_MS = 500\n",
    "MAX_AUDIO_DURATION_S = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a471a-b628-4e42-94e6-ba5afe553758",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 . Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97fe25-e38f-42c0-9c86-d1b9aeb17b9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for text-to-text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de36398-2332-478b-9036-636ba7e595d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/text-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_to_text\u001b[39m(text_generation_input: TextGenerationInput):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load the specified model or use the default one\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text_generation_input\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m!=\u001b[39m gpt2_model_name:\n\u001b[0;32m      5\u001b[0m         model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(text_generation_input\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "@app.post(\"/text-to-text\")\n",
    "async def text_to_text(text_generation_input: TextGenerationInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if text_generation_input.model_name != gpt2_model_name:\n",
    "        model = GPT2LMHeadModel.from_pretrained(text_generation_input.model_name)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(text_generation_input.model_name)\n",
    "    else:\n",
    "        model = gpt2_model\n",
    "        tokenizer = gpt2_tokenizer\n",
    "\n",
    "    # Generate text using the specified model and settings\n",
    "    prompt = text_generation_input.text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids=input_ids.cuda(),\n",
    "                            max_length=256,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(event_id, json.dumps({\"text\": prompt, \"generated_text\": generated_text}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-text_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68430f3c-449d-41c9-9e3b-5d833e9d167f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for text-to-image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9c1ab-6d77-4560-a305-ad093ddd308e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FastAPI endpoint for text-to-image generation\n",
    "@app.post(\"/text-to-image\")\n",
    "async def text_to_image(text_to_image_input: TextToImageInput):\n",
    "    # Generate a unique ID for the event\n",
    "    event_id = str(uuid.uuid4())\n",
    "\n",
    "    # If an image was uploaded, use it, otherwise use the provided text\n",
    "    if text_to_image_input.image is not None:\n",
    "        # Load the image into memory\n",
    "        image_bytes = await text_to_image_input.image.read()\n",
    "        image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        image_tensor = T.ToTensor()(image).unsqueeze(0).cuda()\n",
    "\n",
    "        # Use the text-to-image model to generate an image from the provided text\n",
    "        if text_to_image_input.text is not None:\n",
    "            prompt = f\"{text_to_image_input.text} | image prompt\"\n",
    "        else:\n",
    "            prompt = \"image prompt\"\n",
    "        \n",
    "        # Generate the image using the specified model and settings\n",
    "        output = model.generate(\n",
    "            **tokenizer(prompt, return_tensors=\"pt\").to(image_tensor.device),\n",
    "            do_sample=True,\n",
    "            temperature=text_to_image_input.temperature,\n",
    "            top_p=text_to_image_input.top_p,\n",
    "            max_length=text_to_image_input.max_length,\n",
    "            seed=text_to_image_input.seed,\n",
    "        )\n",
    "        generated_image = T.ToPILImage()(output[0].cpu().clamp(0, 1))\n",
    "    else:\n",
    "        # Generate an image from the provided text\n",
    "        prompt = text_to_image_input.text\n",
    "        \n",
    "        # Generate the image using the specified model and settings\n",
    "        output = model.generate(\n",
    "            **tokenizer(prompt, return_tensors=\"pt\").to(model.device),\n",
    "            do_sample=True,\n",
    "            temperature=text_to_image_input.temperature,\n",
    "            top_p=text_to_image_input.top_p,\n",
    "            max_length=text_to_image_input.max_length,\n",
    "            seed=text_to_image_input.seed,\n",
    "        )\n",
    "        generated_image = T.ToPILImage()(output[0].cpu().clamp(0, 1))\n",
    "\n",
    "    # Save the generated image and text to Redis\n",
    "    if text_to_image_input.text is not None:\n",
    "        redis_client.set(event_id, json.dumps({\"image\": generated_image, \"text\": text_to_image_input.text}))\n",
    "    else:\n",
    "        redis_client.set(event_id, json.dumps({\"image\": generated_image}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-image_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ed37d-bf48-4020-aee7-5c0cb911b10b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Image Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b5b98-d201-4160-97d0-2b909bb5d228",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for image-to-text processing using CLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5295d0-66f1-41db-94a4-9f2fcc63392e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4262625757.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    async def image_to_text(image_to_text_input: ImageToTextInput):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "@app.post(\"/image-to-text\")\n",
    "    async def image_to_text(image_to_text_input: ImageToTextInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if image_to_text_input.model_name != default_clip_model_name:\n",
    "        clip_model = clip.load(image_to_text_input.model_name).eval().cuda()\n",
    "    else:\n",
    "        clip_model = clip.load(default_clip_model_name).eval().cuda()\n",
    "\n",
    "    # Download the image\n",
    "    response = requests.get(image_to_text_input.image_url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    # Process the image using the CLIP model\n",
    "    input_image = T.ToTensor()(image).unsqueeze(0).cuda()\n",
    "    input_text = clip_processor(image_to_text_input.text).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        logits_per_image, logits_per_text = clip_model(input_image, input_text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(event_id, json.dumps({\"image_url\": image_to_text_input.image_url, \"text\": str(probs[0])}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"image-to-text_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd319d-ae79-4984-a2b4-c99bf7d482b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb1794-5afd-4e77-81af-8076f1f6af37",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## FastAPI endpoint for text-to-image generation using diffusion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa6c1b5-8b54-4e44-9453-1ac09954140e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# FastAPI endpoint for text-to-image generation using diffusion models\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/text-to-image-diffuse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_to_image_diffuse\u001b[39m(text_to_image_input: TextToImageInput):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load the specified model or use the default one\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text_to_image_input\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m!=\u001b[39m diffuse_model_name:\n\u001b[0;32m      6\u001b[0m         diffuse_model \u001b[38;5;241m=\u001b[39m Diffusion(prompt_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, diffusion_steps\u001b[38;5;241m=\u001b[39mtext_to_image_input\u001b[38;5;241m.\u001b[39mdiffusion_steps, denoise_scale\u001b[38;5;241m=\u001b[39mtext_to_image_input\u001b[38;5;241m.\u001b[39mdenoise_scale)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "# FastAPI endpoint for text-to-image generation using diffusion models\n",
    "@app.post(\"/text-to-image-diffuse\")\n",
    "async def text_to_image_diffuse(text_to_image_input: TextToImageInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if text_to_image_input.model_name != diffuse_model_name:\n",
    "        diffuse_model = Diffusion(prompt_size=256, image_size=512, diffusion_steps=text_to_image_input.diffusion_steps, denoise_scale=text_to_image_input.denoise_scale)\n",
    "    else:\n",
    "        diffuse_model = Diffusion(prompt_size=256, image_size=512, diffusion_steps=1000, denoise_scale=0.1)\n",
    "\n",
    "    # Generate unique ID for the event\n",
    "    event_id = str(uuid.uuid4())\n",
    "\n",
    "    if text_to_image_input.text is not None:\n",
    "        # Generate an image from the provided text\n",
    "        prompt = text_to_image_input.text\n",
    "\n",
    "        # Generate the image using the specified model and settings\n",
    "        output = diffuse_model.sample(\n",
    "            text=prompt,\n",
    "            clip_model=clip_model,\n",
    "            diffusion_steps=text_to_image_input.diffusion_steps,\n",
    "            temperature=text_to_image_input.temperature,\n",
    "            clip_guidance_scale=text_to_image_input.clip_guidance_scale,\n",
    "            tv_scale=text_to_image_input.tv_scale,\n",
    "            range_scale=text_to_image_input.range_scale,\n",
    "        )\n",
    "        generated_image = Image.fromarray((255 * output.permute(0, 2, 3, 1).cpu().numpy()).astype(np.uint8)[0])\n",
    "\n",
    "        # Save the results to Redis\n",
    "        image_bytes = BytesIO()\n",
    "        generated_image.save(image_bytes, format=\"PNG\")\n",
    "        redis_client.set(event_id, json.dumps({\"text\": prompt, \"image\": base64.b64encode(image_bytes.getvalue()).decode(\"utf-8\")}))\n",
    "    else:\n",
    "        # Generate text using the specified model and settings\n",
    "        prompt = text_to_image_input.image_caption\n",
    "        if prompt is None:\n",
    "            prompt = \"An image generated using text-to-image processing.\"\n",
    "\n",
    "        # Generate the image using the specified model and settings\n",
    "        output = diffuse_model.sample_text(prompt, text_len=text_to_image_input.text_length)\n",
    "        generated_text = output[0][\"text\"]\n",
    "\n",
    "        # Save the results to Redis\n",
    "        redis_client.set(event_id, json.dumps({\"image_caption\": prompt, \"generated_text\": generated_text}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-image-diffuse_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88880176-ba06-40fd-973c-b50f7ae7c898",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## FastAPI endpoint for image-to-image generation using controlnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba60505c-236f-479e-8749-224147859d22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# FastAPI endpoint for image manipulation using ControlNet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/image-manipulation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimage_manipulation\u001b[39m(image_manipulation_input: ImageManipulationInput):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load the specified model or use the default one\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_manipulation_input\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m!=\u001b[39m default_controlnet_model_name:\n\u001b[0;32m      6\u001b[0m         controlnet_model, controlnet_tokenizer \u001b[38;5;241m=\u001b[39m load_controlnet_model(image_manipulation_input\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "# FastAPI endpoint for image manipulation using ControlNet\n",
    "@app.post(\"/image-manipulation\")\n",
    "async def image_manipulation(image_manipulation_input: ImageManipulationInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if image_manipulation_input.model_name != default_controlnet_model_name:\n",
    "        controlnet_model, controlnet_tokenizer = load_controlnet_model(image_manipulation_input.model_name)\n",
    "    else:\n",
    "        controlnet_model, controlnet_tokenizer = load_controlnet_model(default_controlnet_model_name)\n",
    "\n",
    "    # Download the image\n",
    "    response = requests.get(image_manipulation_input.image_url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    # Load the image into memory and encode it as base64\n",
    "    image_bytes = BytesIO()\n",
    "    image.save(image_bytes, format=\"JPEG\")\n",
    "    image_data = base64.b64encode(image_bytes.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Generate a list of attribute-value pairs from the provided text\n",
    "    attribute_values = []\n",
    "    for attribute_value in image_manipulation_input.attribute_values:\n",
    "        attribute, value = attribute_value.split(\":\")\n",
    "        attribute_values.append((attribute.strip(), value.strip()))\n",
    "\n",
    "    # Generate a new image using ControlNet\n",
    "    output_image = generate_image_from_attributes(\n",
    "        controlnet_model,\n",
    "        controlnet_tokenizer,\n",
    "        attribute_values,\n",
    "        source_image=image,\n",
    "        max_num_iterations=image_manipulation_input.max_num_iterations,\n",
    "        save_interval=image_manipulation_input.save_interval,\n",
    "        save_dir=\"./generated_images\",\n",
    "    )\n",
    "\n",
    "    # Load the generated image into memory and encode it as base64\n",
    "    output_bytes = BytesIO()\n",
    "    output_image.save(output_bytes, format=\"JPEG\")\n",
    "    output_data = base64.b64encode(output_bytes.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(\n",
    "        event_id,\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"image_url\": image_manipulation_input.image_url,\n",
    "                \"attribute_values\": image_manipulation_input.attribute_values,\n",
    "                \"output_image\": output_data,\n",
    "                \"input_image\": image_data,\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"image-manipulation_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62bf302-68a4-4f8d-86be-d2acef810640",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Audio Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d15195-8f7a-4aef-bc99-92385a14658b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for text-to-speech conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c011741b-2a7b-419c-b4e7-5bea9b81f50f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# FastAPI endpoint for text-to-speech conversion\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/text-to-speech\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_to_speech\u001b[39m(text_to_speech_input: TextToSpeechInput):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load the specified model or use the default one\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text_to_speech_input\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m!=\u001b[39m default_tts_model_name:\n\u001b[0;32m      6\u001b[0m         tts_model, tts_parser \u001b[38;5;241m=\u001b[39m load_tts_model(text_to_speech_input\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "# FastAPI endpoint for text-to-speech conversion\n",
    "@app.post(\"/text-to-speech\")\n",
    "async def text_to_speech(text_to_speech_input: TextToSpeechInput):\n",
    "    # Load the specified model or use the default one\n",
    "    if text_to_speech_input.model_name != default_tts_model_name:\n",
    "        tts_model, tts_parser = load_tts_model(text_to_speech_input.model_name)\n",
    "    else:\n",
    "        tts_model, tts_parser = load_tts_model(default_tts_model_name)\n",
    "\n",
    "    # Synthesize speech from the provided text\n",
    "    with torch.no_grad():\n",
    "        # Get the phonemes from the text using the text-to-phoneme model\n",
    "        phonemes = g2p(text_to_speech_input.text)\n",
    "\n",
    "        # Convert the phonemes to a tensor\n",
    "        input_ids = torch.LongTensor(tts_parser.text_to_sequence(phonemes)).unsqueeze(0).cuda()\n",
    "\n",
    "        # Synthesize speech using the text-to-speech model\n",
    "        audio = tts_model.inference(input_ids)\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    audio_bytes = audio.cpu().numpy().tobytes()\n",
    "    redis_client.set(event_id, json.dumps({\"text\": text_to_speech_input.text, \"audio\": base64.b64encode(audio_bytes).decode(\"utf-8\")}))\n",
    "    \n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"text-to-speech_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf78b4-d23b-4013-8d34-3302bfcd0f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for speech-to-text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08cf693c-85e4-48f7-8d90-d976e395f3f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Form data requires \"python-multipart\" to be installed. \n",
      "You can install \"python-multipart\" with: \n",
      "\n",
      "pip install python-multipart\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Form data requires \"python-multipart\" to be installed. \nYou can install \"python-multipart\" with: \n\npip install python-multipart\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# FastAPI endpoint for speech-to-text conversion\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129;43m@app\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/speech-to-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;43;01masync\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mspeech_to_text\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUploadFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvad_aggressiveness\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Read the uploaded audio file into memory\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maudio_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_io\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\fastapi\\routing.py:661\u001b[0m, in \u001b[0;36mAPIRouter.api_route.<locals>.decorator\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(func: DecoratedCallable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DecoratedCallable:\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_api_route\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeprecated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeprecated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model_include\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_include\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model_exclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model_by_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_by_alias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model_exclude_unset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude_unset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model_exclude_defaults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude_defaults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model_exclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_in_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_in_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenapi_extra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenapi_extra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerate_unique_id_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_unique_id_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\fastapi\\routing.py:600\u001b[0m, in \u001b[0;36mAPIRouter.add_api_route\u001b[1;34m(self, path, endpoint, response_model, status_code, tags, dependencies, summary, description, response_description, responses, deprecated, methods, operation_id, response_model_include, response_model_exclude, response_model_by_alias, response_model_exclude_unset, response_model_exclude_defaults, response_model_exclude_none, include_in_schema, response_class, name, route_class_override, callbacks, openapi_extra, generate_unique_id_function)\u001b[0m\n\u001b[0;32m    596\u001b[0m     current_callbacks\u001b[38;5;241m.\u001b[39mextend(callbacks)\n\u001b[0;32m    597\u001b[0m current_generate_unique_id \u001b[38;5;241m=\u001b[39m get_value_or_default(\n\u001b[0;32m    598\u001b[0m     generate_unique_id_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_unique_id_function\n\u001b[0;32m    599\u001b[0m )\n\u001b[1;32m--> 600\u001b[0m route \u001b[38;5;241m=\u001b[39m \u001b[43mroute_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_dependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_responses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeprecated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeprecated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeprecated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model_include\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_include\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model_exclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model_by_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_by_alias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model_exclude_unset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude_unset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model_exclude_defaults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude_defaults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model_exclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model_exclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_in_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_in_schema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_in_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_response_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdependency_overrides_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdependency_overrides_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenapi_extra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenapi_extra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_unique_id_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_generate_unique_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroutes\u001b[38;5;241m.\u001b[39mappend(route)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\fastapi\\routing.py:452\u001b[0m, in \u001b[0;36mAPIRoute.__init__\u001b[1;34m(self, path, endpoint, response_model, status_code, tags, dependencies, summary, description, response_description, responses, deprecated, name, methods, operation_id, response_model_include, response_model_exclude, response_model_by_alias, response_model_exclude_unset, response_model_exclude_defaults, response_model_exclude_none, include_in_schema, response_class, dependency_overrides_provider, callbacks, openapi_extra, generate_unique_id_function)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depends \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependant\u001b[38;5;241m.\u001b[39mdependencies\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    450\u001b[0m         get_parameterless_sub_dependant(depends\u001b[38;5;241m=\u001b[39mdepends, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_format),\n\u001b[0;32m    451\u001b[0m     )\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody_field \u001b[38;5;241m=\u001b[39m \u001b[43mget_body_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdependant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdependant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp \u001b[38;5;241m=\u001b[39m request_response(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_route_handler())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\fastapi\\dependencies\\utils.py:844\u001b[0m, in \u001b[0;36mget_body_field\u001b[1;34m(dependant, name)\u001b[0m\n\u001b[0;32m    836\u001b[0m         BodyFieldInfo_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedia_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m body_param_media_types[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    837\u001b[0m final_field \u001b[38;5;241m=\u001b[39m create_response_field(\n\u001b[0;32m    838\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    839\u001b[0m     type_\u001b[38;5;241m=\u001b[39mBodyModel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    842\u001b[0m     field_info\u001b[38;5;241m=\u001b[39mBodyFieldInfo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mBodyFieldInfo_kwargs),\n\u001b[0;32m    843\u001b[0m )\n\u001b[1;32m--> 844\u001b[0m \u001b[43mcheck_file_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_field\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Microserver\\lib\\site-packages\\fastapi\\dependencies\\utils.py:112\u001b[0m, in \u001b[0;36mcheck_file_field\u001b[1;34m(field)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(multipart_not_installed_error)\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(multipart_not_installed_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Form data requires \"python-multipart\" to be installed. \nYou can install \"python-multipart\" with: \n\npip install python-multipart\n"
     ]
    }
   ],
   "source": [
    "# FastAPI endpoint for speech-to-text conversion\n",
    "@app.post(\"/speech-to-text\")\n",
    "async def speech_to_text(audio_file: UploadFile, vad_aggressiveness: int = 3, sample_rate: int = 16000):\n",
    "    # Read the uploaded audio file into memory\n",
    "    audio_bytes = await audio_file.read()\n",
    "    audio_io = BytesIO(audio_bytes)\n",
    "\n",
    "    # Open the audio stream using PyTorch\n",
    "    with torch.no_grad():\n",
    "        # Load the audio waveform from the stream\n",
    "        waveform, sr = torchaudio.load(audio_io)\n",
    "\n",
    "        # Resample the waveform if necessary\n",
    "        if sr != sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert the waveform to mono\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Convert the waveform to the desired format for VAD\n",
    "        vad_input = (waveform * 32768).squeeze().numpy().astype(np.int16)\n",
    "\n",
    "        # Run VAD on the audio to detect speech segments\n",
    "        vad.set_mode(vad_aggressiveness)\n",
    "        frames = np.array_split(vad_input, len(vad_input) // (SILENCE_CHUNK_DURATION_MS * sample_rate // 1000))\n",
    "        speech_segments = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            is_speech = vad.is_speech(frame.tobytes(), sample_rate)\n",
    "            if is_speech:\n",
    "                speech_segments.append(i)\n",
    "\n",
    "        # If no speech was detected, return an empty string\n",
    "        if not speech_segments:\n",
    "            return {\"text\": \"\"}\n",
    "\n",
    "        # Extract the speech segments from the audio\n",
    "        speech_audio = torch.cat([waveform[:, segment * (SILENCE_CHUNK_DURATION_MS * sample_rate // 1000):(segment + 1) * (SILENCE_CHUNK_DURATION_MS * sample_rate // 1000)] for segment in speech_segments], dim=-1)\n",
    "\n",
    "        # Transcribe the speech to text using the specified model\n",
    "        transcribed_text = model(speech_audio, sample_rate=sample_rate)\n",
    "\n",
    "    # Generate a unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    redis_client.set(event_id, json.dumps({\"audio\": base64.b64encode(audio_bytes).decode(\"utf-8\"), \"text\": transcribed_text}))\n",
    "\n",
    "    # Publish the event using Redis Pub/Sub\n",
    "    redis_client.publish(\"speech-to-text_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id, \"text\": transcribed_text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc1efd-4c24-463a-9249-583bcf5eb2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FastAPI endpoint for speech-to-speech conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec034d-fd6a-4bdc-b28f-af4a0d9f4e9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# FastAPI endpoint for speech-to-speech conversion\n",
    "@app.post(\"/speech-to-speech\")\n",
    "async def speech_to_speech(speech_to_speech_input: SpeechToSpeechInput):\n",
    "    # Load the specified models or use the default ones\n",
    "    if speech_to_speech_input.source_language != \"en\":\n",
    "        stt_model, stt_parser = load_stt_model(speech_to_speech_input.source_language)\n",
    "    else:\n",
    "        stt_model, stt_parser = load_stt_model(default_stt_model_name)\n",
    "\n",
    "    if speech_to_speech_input.target_language != \"en\":\n",
    "        tts_model, tts_parser = load_tts_model(speech_to_speech_input.target_language)\n",
    "    else:\n",
    "        tts_model, tts_parser = load_tts_model(default_tts_model_name)\n",
    "\n",
    "    # Transcribe the audio from the source language\n",
    "    audio_bytes = await speech_to_speech_input.audio_file.read()\n",
    "    audio_tensor, _ = torchaudio.load(BytesIO(audio_bytes))\n",
    "    if audio_tensor.size(0) > 1:\n",
    "        audio_tensor = audio_tensor.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample the audio if necessary\n",
    "    if audio_tensor.shape[1] != stt_parser.sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(audio_tensor.shape[1], stt_parser.sample_rate)\n",
    "        audio_tensor = resampler(audio_tensor)\n",
    "\n",
    "    # Detect speech segments\n",
    "    speech_segments = detect_speech(audio_tensor, stt_parser.sample_rate)\n",
    "\n",
    "    # Transcribe the speech segments\n",
    "    transcriptions = []\n",
    "    for segment in speech_segments:\n",
    "        segment_tensor = audio_tensor[:, segment[0]:segment[1]]\n",
    "        with torch.no_grad():\n",
    "            input_signal = stt_parser.preprocess(segment_tensor)\n",
    "            input_signal = input_signal.cuda()\n",
    "            _, predicted = stt_model(input_signal, input_signal.new_zeros([input_signal.shape[0], 1], dtype=torch.long))\n",
    "            transcription = stt_parser.decode(predicted[0])\n",
    "            transcriptions.append(transcription)\n",
    "\n",
    "    # Translate the transcriptions to the target language\n",
    "    translations = []\n",
    "    for transcription in transcriptions:\n",
    "        translation = translator.translate(transcription, src=speech_to_speech_input.source_language, dest=speech_to_speech_input.target_language)\n",
    "        translations.append(translation.text)\n",
    "\n",
    "    # Synthesize speech in the target language\n",
    "    with torch.no_grad():\n",
    "        # Get the phonemes from the translated text using the text-to-phoneme model\n",
    "        phonemes = g2p(translations)\n",
    "\n",
    "        # Convert the phonemes to a tensor\n",
    "        input_ids = torch.LongTensor(tts_parser.text_to_sequence(phonemes)).unsqueeze(0).cuda()\n",
    "\n",
    "        # Synthesize speech using the text-to-speech model\n",
    "        audio = tts_model.inference(input_ids)\n",
    "\n",
    "    # Generate unique ID for the event and save the results to Redis\n",
    "    event_id = str(uuid.uuid4())\n",
    "    audio_bytes = audio.cpu().numpy().tobytes()\n",
    "    redis_client.set(event_id, json.dumps({\"audio\": base64.b64encode(audio_bytes).decode(\"utf-8\")}))\n",
    "    redis_client.publish(\"speech-to-speech_events\", event_id)\n",
    "\n",
    "    return {\"event_id\": event_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f88d0-41c0-4939-9ff7-150ddda2a34c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. Match Moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c111040-c243-4fb5-8a86-984f07ea5f31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ebc2e8-3e3d-4776-a8ee-820ba4f88276",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Run the FastAPI application in the Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4c362-3911-4347-9fd2-da82d9e2b6b9",
   "metadata": {},
   "source": [
    "> Note that the pyngrok package is optional and only needed if you want to expose your API to the internet using ngrok. To install the pyngrok package, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be202e-f5f4-4bb9-99cc-7530028f9983",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install pyngrok\n",
    "# pyngrok :\n",
    "try:\n",
    "    import pyngrok                      \n",
    "    print('pyngrok: already installed')\n",
    "except ImportError:\n",
    "  !python -m pip install -q pyngrok\n",
    "  print('Installed pyngrok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846934e4-3146-4bd8-9f14-61f9e78ad549",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up ngrok for external access (optional)\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
    "\n",
    "# Run the FastAPI app\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f5336-a431-41e6-a48f-4d602b98d065",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
